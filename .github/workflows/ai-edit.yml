name: AI Edit (Claude Code)

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "The user's edit request"
        required: true
        type: string
      model:
        description: "Claude model to use"
        required: false
        default: "claude-sonnet-4-5-20250929"
        type: string

jobs:
  ai-edit:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Scrape reference sites
        timeout-minutes: 5
        continue-on-error: true
        env:
          PROMPT_TEXT: ${{ github.event.inputs.prompt }}
        run: |
          mkdir -p reference
          # Extract URLs from the prompt (only scrape the FIRST URL as the main site)
          FIRST_URL=$(echo "$PROMPT_TEXT" | grep -oP 'https?://[^\s"'"'"'<>]+' | head -1 || true)
          URLS="$FIRST_URL"
          if [ -n "$URLS" ]; then
            for URL in $URLS; do
              DOMAIN=$(echo "$URL" | sed 's|https\?://||' | sed 's|/.*||')
              echo "Scraping $URL ..."

              # Step 1: Fetch main page and discover all internal links
              curl -sL --max-time 10 "$URL" > "reference/main-page.html" 2>/dev/null || true
              SUBPAGES=$(curl -sL --max-time 10 "$URL" | grep -oP 'href="(https?://'"$DOMAIN"'[^"]*|/[^"]*)"' | sed 's/href="//;s/"//' | sort -u | head -30)

              # Step 2: Fetch each subpage HTML
              for SUB in $SUBPAGES; do
                case "$SUB" in
                  http*) FULL_URL="$SUB" ;;
                  *)     FULL_URL="$URL$SUB" ;;
                esac
                SAFE_NAME=$(echo "$SUB" | tr '/:?&#' '______' | head -c 100)
                echo "  Fetching: $FULL_URL"
                curl -sL --max-time 8 "$FULL_URL" > "reference/page_$SAFE_NAME.html" 2>/dev/null || true
              done

              # Step 3: Download images from the site (just images, skip JS/CSS/fonts)
              mkdir -p "reference/images"
              grep -ohP '(src|srcset)="(https?://[^"]*\.(jpg|jpeg|png|gif|svg|webp)[^"]*)"' reference/*.html 2>/dev/null | \
                grep -oP 'https?://[^"]+' | sort -u | head -40 | while read IMG_URL; do
                  FNAME=$(basename "$IMG_URL" | cut -d'?' -f1 | head -c 80)
                  echo "  Image: $FNAME"
                  curl -sL --max-time 5 "$IMG_URL" -o "reference/images/$FNAME" 2>/dev/null || true
                done

              echo "Done scraping $DOMAIN"
            done
          fi
          echo "=== Reference files ==="
          find reference/ -type f 2>/dev/null | head -100 || echo "No reference files"
          echo "=== Total size ==="
          du -sh reference/ 2>/dev/null || true

      - name: Install Claude Code
        run: |
          npm install -g @anthropic-ai/claude-code
          # Gitignore reference/ BEFORE Claude runs so it won't commit scraped files
          echo "reference/" >> .gitignore

      - name: Run Claude
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          PROMPT_TEXT: ${{ github.event.inputs.prompt }}
          MODEL_NAME: ${{ github.event.inputs.model }}
        run: |
          claude --model "$MODEL_NAME" --dangerously-skip-permissions --max-turns 50 -p "
          The user wants you to edit their website. Here is their request:

          $PROMPT_TEXT

          IMPORTANT INSTRUCTIONS:
          1. Read CLAUDE.md first — it contains CRITICAL rules you must follow.

          2. If there is a reference/ directory, it contains a SCRAPED COPY of the user's
             existing site or reference site. READ EVERY HTML FILE in reference/ carefully.
             Extract ALL real content: text, headings, descriptions, addresses, phone numbers,
             apartment listings, team members, services, prices — EVERYTHING.

          3. For images: check reference/images/ for downloaded images. Copy them to the
             site's images/ folder. If images weren't downloaded, hotlink the original URLs.

          4. If the user linked to an existing site to recreate:
             - You MUST reproduce ALL the content and functionality, not just the homepage
             - Check EVERY subpage HTML file in reference/
             - Include all navigation links, all sections, all data
             - If the site has listings (apartments, products, etc.) — include ALL of them
             - If links go to external sites, keep those as external links
             - Recreate the COMPLETE site, not a summary of it

          5. ABSOLUTE RULE — NO INCOMPLETE PAGES:
             - NEVER create a page that says 'under construction' or 'coming soon'
             - NEVER create stub/placeholder pages
             - If you cannot fully complete a page with real content, DO NOT create it
             - Remove incomplete pages from navigation entirely
             - A missing page is 1000x better than a placeholder page
             - Only link to pages that exist and are fully complete

          6. UI QUALITY RULES:
             - Navbar must be flush to the very top of the page (top-0, no gaps/margins)
             - Dropdown menus MUST work on hover — nest the dropdown inside the trigger's
               parent element so the hover state stays active when moving to dropdown items.
               Use Tailwind group/group-hover. NO gap between trigger and dropdown content.
             - Every interaction must work: hovers, clicks, dropdowns, mobile menu
             - Test every link points to a real, complete page

          7. After making all changes, commit them with a descriptive message using git.
             Do NOT commit the reference/ directory.
          "

      - name: Push changes
        run: |
          git config user.name "claude[bot]"
          git config user.email "noreply@anthropic.com"
          git add -A
          git diff --staged --quiet || git commit -m "AI edit via SiteForge"
          git push
