name: AI Edit (Claude Code)

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "The user's edit request"
        required: true
        type: string
      model:
        description: "Claude model to use"
        required: false
        default: "claude-sonnet-4-5-20250929"
        type: string

jobs:
  ai-edit:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Scrape reference sites
        timeout-minutes: 5
        run: |
          mkdir -p reference
          # Extract URLs from the prompt
          URLS=\$(echo "\${{ github.event.inputs.prompt }}" | grep -oP 'https?://[^\\s"'"'"'<>]+' || true)
          if [ -n "\$URLS" ]; then
            for URL in \$URLS; do
              DOMAIN=\$(echo "\$URL" | sed 's|https\\?://||' | sed 's|/.*||')
              echo "Scraping \$URL ..."

              # Step 1: Fetch main page and discover all internal links
              curl -sL "\$URL" > "reference/main-page.html" 2>/dev/null || true
              SUBPAGES=\$(curl -sL "\$URL" | grep -oP 'href="(https?://'""\$DOMAIN""'[^"]*|/[^"]*)"' | sed 's/href="//;s/"//' | sort -u | head -30)

              # Step 2: Fetch each subpage HTML
              for SUB in \$SUBPAGES; do
                case "\$SUB" in
                  http*) FULL_URL="\$SUB" ;;
                  *)     FULL_URL="\$URL\$SUB" ;;
                esac
                SAFE_NAME=\$(echo "\$SUB" | tr '/:?&#' '______' | head -c 100)
                echo "  Fetching: \$FULL_URL"
                curl -sL --max-time 8 "\$FULL_URL" > "reference/page_\$SAFE_NAME.html" 2>/dev/null || true
              done

              # Step 3: Download images from the site (just images, skip JS/CSS/fonts)
              mkdir -p "reference/images"
              # Extract image URLs from all downloaded HTML
              grep -ohP '(src|srcset)="(https?://[^"]*\.(jpg|jpeg|png|gif|svg|webp)[^"]*)"' reference/*.html 2>/dev/null | \
                grep -oP 'https?://[^"]+' | sort -u | head -40 | while read IMG_URL; do
                  FNAME=\$(basename "\$IMG_URL" | cut -d'?' -f1 | head -c 80)
                  echo "  Image: \$FNAME"
                  curl -sL --max-time 5 "\$IMG_URL" -o "reference/images/\$FNAME" 2>/dev/null || true
                done

              echo "Done scraping \$DOMAIN"
            done
          fi
          echo "=== Reference files ==="
          find reference/ -type f 2>/dev/null | head -100 || echo "No reference files"
          echo "=== Total size ==="
          du -sh reference/ 2>/dev/null || true

      - name: Install Claude Code
        run: npm install -g @anthropic-ai/claude-code

      - name: Run Claude
        env:
          ANTHROPIC_API_KEY: \${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          claude --model "\${{ github.event.inputs.model }}" --dangerously-skip-permissions --max-turns 30 -p "
          The user wants you to edit their website. Here is their request:

          \${{ github.event.inputs.prompt }}

          IMPORTANT INSTRUCTIONS:
          1. Read CLAUDE.md first for the site editing rules.

          2. If there is a reference/ directory, it contains a SCRAPED COPY of the user's
             existing site or reference site. READ EVERY HTML FILE in reference/ carefully.
             Extract ALL real content: text, headings, descriptions, addresses, phone numbers,
             apartment listings, team members, services, prices — EVERYTHING.

          3. For images: check reference/ for downloaded images. Copy any useful images to the
             site root (or an images/ folder). If images weren't downloaded, keep the original
             URLs from the source site as img src (hotlink them).

          4. If the user linked to an existing site to recreate:
             - You MUST reproduce ALL the content and functionality, not just the homepage
             - Check EVERY subpage HTML file in reference/
             - Include all navigation links, all sections, all data
             - If the site has listings (apartments, products, etc.) — include ALL of them
             - If links go to external sites, keep those as external links
             - Recreate the COMPLETE site, not a summary of it

          5. After making all changes, commit them with a descriptive message using git.
          "

      - name: Push changes
        run: |
          git config user.name "claude[bot]"
          git config user.email "noreply@anthropic.com"
          # Don't push the reference/ scrape folder
          echo "reference/" >> .gitignore
          git add -A
          git diff --staged --quiet || git commit -m "AI edit via SiteForge"
          git push
